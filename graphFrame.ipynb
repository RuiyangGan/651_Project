{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with graphframe and pyspark\n",
    "\n",
    "I tend to follow the User guide for Graphframe and that is posted on Databricks https://docs.databricks.com/spark/latest/graph-analysis/graphframes/user-guide-python.html. \n",
    "\n",
    "First, we will import the packages needed to construct the graph data structure in the Graphframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark and Graphframe\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "from graphframes import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, read the fork edges and the contribution edges from the text file `fork_edges.txt` and `contrib_edges.txt`. For each line in the two files, the edges will be in the form of a tuple `(u,r)` or `(r,u)`. Here `r` and `u` specifies the repositories ID and user ID on GitHub. For more information about the storage formats of `fork_edges.txt` and `contrib_edges.txt`. and how edges are collected (and how the network is formed), please consult the meta file `meta.md`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use literal eval to parse the string as tuple\n",
    "from ast import literal_eval\n",
    "\n",
    "# Load the contribution edges and the fork edges\n",
    "with open('contrib_edges.txt', 'r') as f1:\n",
    "    contrib_edges = f1.readlines()\n",
    "    \n",
    "with open('fork_edges.txt', 'r') as f2:\n",
    "    fork_edges = f2.readlines()\n",
    "    \n",
    "# Parse string literal to become tuples\n",
    "contrib_edges = [literal_eval(t) if len(t.strip('\\n')) != 0 else None\n",
    "                 for t in contrib_edges]\n",
    "fork_edges = [literal_eval(t) if len(t.strip('\\n')) != 0 else None\n",
    "              for t in fork_edges]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we read in the contribution edges and fork edges, we will build the Biparitite Graph using Graphframe. First, we will construct two data frames: `vertices` and `edges`. Using these two tables, we will construct a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary EDA on the Graph\n",
    "In-degree, out-degree, degree distribution of repos, users..., power-law?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Rank of GitHub's repository and users\n",
    "Top ten influential repository?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Spectrum\n",
    "Since we don't have the direct collaboration information between users (which requires push access to specific repo), we are only able to collect the contributors of specific repos and users who fork the repos. Thus the contribution-fork edge network is bipartite.\n",
    "\n",
    "Let $$ A $$ be the Adjacency matrix of GitHub's contribution-fork network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Detection\n",
    "Implement community detection algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (If Time allows) Matrix completion problem\n",
    "We now observe a bipartite contribution-fork network. But we know that users on GitHub collaborate with each other (or they can follow someone they look up to) and repos depends on other repos as well (unless developers likes to torture themselves or tired of being tortured by shitty packages they use on a daily basis). We can think of this as a matrix completion problem, where we want to fill in the unobserved entries in our population adjacency matrix. Suppose we have a small sample of collaboration/follower and repository dependency information, can we use that to complete our population graph?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
