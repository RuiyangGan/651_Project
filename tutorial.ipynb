{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Large-Scale Network Analysis with Pyspark and Graphframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "This tutorial provides an introduction on how to conduct large-scale network analysis using Spark and graphframe. This tutorial will focus on basic features of Pyspark SQL that can be applied toward computation on large graph. Features of graphframe (a package in Spark that focuses on graph-related computation) will also be introduced. \n",
    "\n",
    "Before we proceed to the rest of this tutorial, I will provide some background of Spark. Spark is a computing framework developed by Apache Foundation in 2013 to overcome some shortcoming of Apache Hadoop MapReduce. General pipeline of MapReduce is as follow: apply function to some data on disk and assign a key (map), group data by batch and distribute batches of data to computing nodes (shuffle), aggregate the batch of data to a single result (reduce) and store on Disk. This process is slow as it relies on disk operation (streaming batch of data to individual computing node and reading into disk for further computing job) and somewhat cumbersome (for each computing job, a mapper and reducer has to be provided). Spark is faster than the Hadoop MapReduce for most of the computing tasks as it is optimized for such computation. Also, since the introduction of Data Frame and Spark SQL since Spark 1.6, such cumbersomeness will also be alleviated as some of the computing jobs can be expressed in SQL expressions. For example, a simple word count program can be done by just `wordDF.groupby('word').count()` given all words are parsed and stored in a data frame called `wordDF` with column `word`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Environment and IO\n",
    "After a rough introduction of Spark, we will begin the tutorial by setting up Spark Environment. Spark has api for Scala, Python, and R. For this tutorial, I will use Spark's api for Python, which is Pyspark. Installing Pyspark is easy as it is available in PyPi. To install, just type `pip install pyspark` in your shell, as it will automatically install the dependency and add pyspark in the `PATH`. For installation of other api, I suggest reading https://spark.apache.org/downloads.html for downloads and installation. \n",
    "\n",
    "To test whether Spark is properly installed, type `pyspark` in your shell. If you see the following message pop up in your shell:\n",
    "``` shell\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.0\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.7.0 (default, Jun 28 2018 13:15:42)\n",
    "SparkSession available as 'spark'\n",
    "```\n",
    "Then you have pyspark correctly installed and configured.\n",
    "\n",
    "After installing pyspark, we will first run Spark in local mode. To use Spark, we will need an entry point. Such entry point will be a `SparkSession` object (given that we deal mostly with DataFrames). Note that in the above shell message, a default `SparkSession` object is created as `spark`. Now, we will show how to create one `SparkSession` with desired configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "# Create configuration for Spark Session\n",
    "conf = SparkConf() \\\n",
    "    .setAll([('spark.executor.memory', '16g'),\n",
    "             ('spark.executor.cores', '8'),\n",
    "             ('spark.cores.max', '8'),\n",
    "             ('spark.driver.memory','16g'),\n",
    "             ('spark.sql.execution.arrow.enabled', True)])\n",
    "\n",
    "# Create a spark session\n",
    "demo = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code snippet above, note that we add in the key-value pair `('spark.sql.execution.arrow.enabled', True)` into our Spark config. Apache Arrow is a standardized in-memory data storage format. A noticeable feature of Arrow is that it is language-independent. As the data are stored in memory in Arrow format, there is no conversion overhead for data transferring between python and Spark. This configuration is also essential to our next steps, as it enables the use of pandas udf (pandas user defined function). \n",
    "\n",
    "After installing Pyspark, let's try creating some DataFrames in Spark. Pyspark SQL accepts multiple formats of input, such as csv, json, pandas, parquet, etc. We will try reading data from parquet format. Parquet is a columnar, binary storage format. It is the on-disk storage format for the Arrow format we mentioned above. We can convert a csv/txt file to a parquet file by first converting it to a pandas dataframe and then convert it to parquet. The following code snippet describe the steps of converting a text file to a parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Read from csv to create a pandas data frame\n",
    "edges_pd = pd.read_csv('data/edges.csv', columns = ['src', 'dst'])\n",
    "edges_pd.to_parquet('data/edges.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should have a file named `edges.parquet` with column `src` and `dst` in a directory `data` (Create one beforehand if you don't have one already). Now, let's try reading this parquet file into our current Spark Session as a data.frame and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "| src|dst|\n",
      "+----+---+\n",
      "| -35| 27|\n",
      "| -28|263|\n",
      "|-165|100|\n",
      "| -26|263|\n",
      "| -65|134|\n",
      "| -26|134|\n",
      "| -65| 90|\n",
      "|-144| 75|\n",
      "|-732|242|\n",
      "|-249|475|\n",
      "+----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df = demo.read.parquet('data/edges.parquet')\n",
    "edges_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark SQL and Pandas UDF\n",
    "After we load our `edges` dataframe into our Spark session, we shall now exlpore more features of the pyspark sql. Since the first data frame `edges` is in the form of an adjacency list, we can do some very basic graphic EDA using SQL. Now suppose that all positive number in the `edges` data frame is a GitHub user's ID, while every negative number represents the ID of a GitHub repository. Then each row in our `edges` table represents an edge from `src` to `dst` in this simple graph. Next, we want to know what are the indegrees and outdegrees of repositories on GitHub. It turns out that it is quite easy to do so with Spark SQL. All we need is three table operations: `filter()`, `groupby()`, and `count()`. Let's take the calculation of indegree of repositories as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|    id|inDegree|\n",
      "+------+--------+\n",
      "|  -882|       1|\n",
      "| -1299|       1|\n",
      "| -3150|       1|\n",
      "| -4657|       1|\n",
      "| -8001|       9|\n",
      "| -8659|       5|\n",
      "|-10751|       1|\n",
      "|-10798|       1|\n",
      "|-12237|      20|\n",
      "|-16813|       8|\n",
      "+------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "repo_indegree = edges_df.filter('dst < 0') \\\n",
    "    .groupby('dst') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('dst', 'id') \\\n",
    "    .withColumnRenamed('count', 'inDegree').cache()\n",
    "repo_indegree.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will break down the above code snippet. First, we filter out the entries where the destination positive (only keeping negative number which represents the repositories). Next, we group by the destination nodes and aggregate each grouped data by counting the number of rows in each group. Using this `repo_indegree ` table, we can check the degree distribtion of GitHub's repo's contribution. Using the function `approxQuantile` provided in pyspark.sql, we can calculate the approximate empirical cdf of the indegree for the repositories on GitHub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in log10\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "samp_probs = np.linspace(1e-4, 1, int(1e4)).tolist()\n",
    "degree_quantile = repo_indegree.stat \\\n",
    "            .approxQuantile(col='inDegree',\n",
    "                            probabilities=samp_probs,\n",
    "                            relativeError=1e-4)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(np.log10(degree_quantile),\n",
    "         np.log10([1 - i for i in samp_probs]),\n",
    "                'bo')\n",
    "plt.xlabel('Base-10 Log of Degree')\n",
    "plt.ylabel('Base-10 Log of Probability')\n",
    "plt.title('Degree distribution on Base-10 Log-Log scale')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although pyspark.sql.functions provide a wide range of function library, in some cases we may find that none of the function provided fits our need. For example, in our following graphframe example, we want to have an aggregate function that returns most frequent item within a list, and if there are ties, choose uniformly random on the set of most frequent items. The existed spark sql function library doesn't seem to have such function. Now, we will need the help of Pandas UDF (User defined function). A User Defined Function (udf) in pyspark.sql is a function operation on the columns of data frame converted from a Python function that the user define. A pandas udf is a special type of udf, as it takes in `pandas.DataFrame` or `pandas.Series` and it allows the action of aggregate on columns, which is not achievable by the function `udf` provided in `pyspark.sql.functions`. It is also claimed to be optimized against the usual `udf`. There are three types of pandas udf, `SCALAR`, `GROUPED_MAP`, `GROUPED_AGG`. `SCALAR` pandas udf is a similar type of udf as `udf` provided in `pyspark.sql.functions`, as it takes in a column and return another column of the same length. `GROUPED_MAP`  is a type of udf that goes in hand with `groupby` operation, as it applies (map) a function on every observation within grouped data. `GROUPED_AGG` is an aggregate operation (reduce) within grouped data. This is the type of pandas udf that I will focus on here, as we need a function that reduces a list of items to one single value by returning the most frequent item. (_For a thorough tutorial on these three types of functions, I suggest reading this tutorial at https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html._) Now, let's look at the code snippet of our pandas UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Numbers|\n",
      "+-------+\n",
      "|      1|\n",
      "|      2|\n",
      "|      3|\n",
      "|      2|\n",
      "|      2|\n",
      "+-------+\n",
      "\n",
      "+------------------+\n",
      "|Most Frequent item|\n",
      "+------------------+\n",
      "|                 2|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "@pandas_udf(\"int\", PandasUDFType.GROUPED_AGG)\n",
    "def maxLabel_udf(label_list):\n",
    "    # Filter out the Nones in the list\n",
    "    label_list = list(filter(None, label_list))\n",
    "    # Count the number of occurences for each item in list\n",
    "    LabelCounts = Counter(label_list)\n",
    "    # Retrieve the most frequent items in this list\n",
    "    mostCommonLabels = [i[0] for i in LabelCounts.items()\n",
    "                        if i[1] == max(LabelCounts.values())]\n",
    "    # If there are multiple most frequent items, randomly draw one\n",
    "    # and return\n",
    "    return int(np.random.choice(mostCommonLabels))\n",
    "\n",
    "# Create a toy data frame for testing purpose\n",
    "udf_test = demo.createDataFrame([(1,), (2,), (3,), (2,), (2,)], ['Numbers',])\n",
    "udf_test.show()\n",
    "\n",
    "# Apply the pandas udf to the above data frame\n",
    "udf_test.groupby() \\\n",
    "    .agg(maxLabel_udf(F.col('Numbers')) \\\n",
    "    . alias('Most Frequent item')).show()\n",
    "\n",
    "# The udf returns 2 as the most frequent item, which is the\n",
    "# correct answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code snippet above, we first use a decorator `@pandas_udf(\"int\", PandasUDFType.GROUPED_AGG)` to define this udf to be an grouped aggregate function and the return type of this udf to be integer. In the next line, we just defined an ordinary python function that returns the most frequent label. Next, we just need to apply the function over the `Numbers` column. It seems like our pandas_udf returns the correct value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphframe\n",
    "After a brief introduction to the Pyspark SQL and pandas_udf, we will move our focus to graphframes and some of its basic functionality. In Spark's native library, there is a component named GraphX, which specializes for graph processing. In essential, GraphX represents a graph strcutre in the form of GraphRDD, which is a wrapper on top of VertexRDD and EdgeRDD (As every graph can be represented by a set of certices and a set of edges, and these two sets are stored in the form of RDD in Spark). The package includes algorithms such as BFS, DFS, pageRank, Label Propagation. It also provides an API for Pregel. However, GraphX doesn't have a python API. Thus, python user can not use the functionality included in GraphX for graph processing. \n",
    "\n",
    "Fortunately, there is another option for Python users which is called graphframes. It builds upon a similar idea as GraphX. Instead of using RDD to store the vertices and edges, graphframes uses dataframe as the form of storing the vertices and edges. graphframes comes with almost the same package of built-in algorithms as GraphX (in fact, these functions are directly translated from GraphX as RDD can be converted into DataFrames). Most importantly, it comes with a Python API so that Python process can get access to Java objects through Py4j. \n",
    "\n",
    "Remember that we have loaded an data frame called `edges_df`, which contains all the information between GitHub's users and repos. Now, we will load a data frame that represents all the nodes in our GitHub network called `vertices_df`. Then, we will create a graphframe object in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|    id|inDegree|\n",
      "+------+--------+\n",
      "|  -882|       1|\n",
      "| -1299|       1|\n",
      "| -3150|       1|\n",
      "| -4657|       1|\n",
      "| -8001|       9|\n",
      "| -8659|       5|\n",
      "|-10751|       1|\n",
      "|-10798|       1|\n",
      "|-12237|      20|\n",
      "|-16813|       8|\n",
      "+------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from graphframes import GraphFrame\n",
    "\n",
    "# Read the vertices data frame\n",
    "vertices_df = demo.read.parquet('data/vertices.parquet')\n",
    "# Create a graphframe object\n",
    "graphframe_demo = GraphFrame(vertices_df, edges_df)\n",
    "# Only keep the edges where the destination is a repo\n",
    "# and show the inDegrees of the filtered graph\n",
    "graphframe_demo.filterEdges('dst < 0') \\\n",
    "    .inDegrees.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code snippet above, we read in the vertices list from a parquet file to create a data frame called `vertices_df`. Using the `vertices_df` and `edges_df`, we created a graphframe object called `graphframe_demo`. The graphframe object has built-in functionalities considered useful. Getting back to the example where we calculate the in degrees of GitHub's repository. A `graphframe` object already has attributes `inDegrees` stored when the object is being constructed. Now, we will just need to use the function `filterEdges` to obtain the subgraph that only contains edges from users to repositories in our GitHub use-repo example and access the `inDegrees` of the subgraph. We can see that it produce the same result as in previous example.\n",
    "\n",
    "graphframes also comes with common graph algorithms. Suppose we are interested in the influential (popular) GitHub repositories, we can use the included `pageRank` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageRank_demo = graphframe_demo.pageRank(maxIter=10).vertices.cache()\n",
    "top_10_repo = pageRank_demo \\\n",
    "        .filter('id < 0') \\\n",
    "        .orderBy(pageRank_demo.pagerank.desc()) \\\n",
    "        .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Page Rank</th>\n",
       "      <th>Repo name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10594164</td>\n",
       "      <td>346.344554</td>\n",
       "      <td>prism-break/prism-break</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17839626</td>\n",
       "      <td>275.924734</td>\n",
       "      <td>HGustavs/LenaSYS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-19726406</td>\n",
       "      <td>261.824003</td>\n",
       "      <td>strategist922/datasciencecoursera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-14681712</td>\n",
       "      <td>253.918046</td>\n",
       "      <td>jacano1969/MVC_framework</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-20329618</td>\n",
       "      <td>244.805479</td>\n",
       "      <td>skillcrush/skillcrush-104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-18491419</td>\n",
       "      <td>240.140818</td>\n",
       "      <td>UIKit0/roslyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-19369035</td>\n",
       "      <td>231.188696</td>\n",
       "      <td>TEAMMATES/teammates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1175581</td>\n",
       "      <td>207.059437</td>\n",
       "      <td>citation-style-language/styles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-18524934</td>\n",
       "      <td>206.130144</td>\n",
       "      <td>Roll20/roll20-character-sheets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-10614101</td>\n",
       "      <td>204.298904</td>\n",
       "      <td>TheOdinProject/curriculum</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id   Page Rank                          Repo name\n",
       "0 -10594164  346.344554            prism-break/prism-break\n",
       "1 -17839626  275.924734                   HGustavs/LenaSYS\n",
       "2 -19726406  261.824003  strategist922/datasciencecoursera\n",
       "3 -14681712  253.918046           jacano1969/MVC_framework\n",
       "4 -20329618  244.805479          skillcrush/skillcrush-104\n",
       "5 -18491419  240.140818                      UIKit0/roslyn\n",
       "6 -19369035  231.188696                TEAMMATES/teammates\n",
       "7  -1175581  207.059437     citation-style-language/styles\n",
       "8 -18524934  206.130144     Roll20/roll20-character-sheets\n",
       "9 -10614101  204.298904          TheOdinProject/curriculum"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the names of the repositories by IDs\n",
    "from github import Github\n",
    "from configparser import ConfigParser\n",
    "import pandas as pd\n",
    "\n",
    "parser = ConfigParser()\n",
    "parser.read('github.auth')\n",
    "user_1 = parser.sections()[3]\n",
    "params = parser.items(user_1)\n",
    "auth = {}\n",
    "for param in params:\n",
    "    auth[param[0]] = param[1]\n",
    "g = Github(**auth)\n",
    "\n",
    "repo_names = []\n",
    "for repo in top_10_repo:\n",
    "    repo_names.append(g.get_repo(-repo[0]).full_name)\n",
    "\n",
    "top_10_repo = pd.concat([pd.DataFrame(top_10_repo, columns = ['Id', 'Page Rank']),\n",
    "                         pd.DataFrame(repo_names, columns = ['Repo name',])],\n",
    "                         axis = 1)\n",
    "top_10_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message Passing\n",
    "\n",
    "Now we have built a graphframe object in Spark and being able to conduct some network analysis with functionalities included in graphframe. Suppose we want to develop our own graph algorithms, what tools can graphframe and spark offer us? It turns out that graphframes' message passing API `aggregateMessages` and its related utility function library `AggregateMessages` are very powerful tools. Message passing builds upon the idea of 'thinking like a node'. Graph algorithms can then be transformed into \n",
    "    1. Nodes exchanging information through the links between each other.\n",
    "    2. Aggregate information received at every node.\n",
    "    3. Repeat the above steps until some stopping condition\n",
    "    \n",
    "Next, we will look at one example of how message passing is deployed. Suppose we want to write our own label propagation algorithm that is designed for bipartite network, especially for our GitHub network, how should we do that in Spark? Before diving into the code, I will briefly describe what is Label Propagation algorithm. It is intended for community detection in complex networks. The idea of the algorithm is simple: \n",
    "    Initially every node get its own label\n",
    "    for each iteration, do:\n",
    "        1. Every node send its labels to other nodes.\n",
    "        2. Each node pick the most frequent label from its labels; \n",
    "        If there are ties, choose one label uniformly at random.\n",
    "\n",
    "It is an efficient algorithm as for each node only needs to pick the most frequent label. And it is embarrassingly parallel. Based on the above algorithm, we now want to make some modification to better fit our GitHub network. Instead of assigning labels to every user and repo, we now want to assign every user a distinct label. And users send the labels to the repos that they have contributed. After repos received labels from the users, repos will then send the labels to its contributors and users that have forked from the repo. This alternating updating schema repeat until the desired number of iteration is completed. \n",
    "\n",
    "Time to look at some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes.lib import AggregateMessages as AM\n",
    "\n",
    "# Determine node type of the vertices; 1 is user, 2 is repo\n",
    "nodeTypeUDF = F.udf(lambda i: 1 if i > 0 else 2, types.IntegerType())\n",
    "vertices_df = vertices_df.withColumn('nodeType', nodeTypeUDF(F.col('id')))\n",
    "\n",
    "# Assign initial label to the users\n",
    "initLabelUDF = F.udf(lambda i, j: i if j == 1 else None,\n",
    "                     types.IntegerType())\n",
    "v = graphframe_demo.vertices.withColumn('label',\n",
    "        initLabelUDF(F.col('id'), F.col('nodeType')))\n",
    "\n",
    "# Add edges for every node that goes to itself\n",
    "E_self = demo.createDataFrame(v.select(F.col('id')).rdd)\n",
    "E = graphframe_demo.edges.union(\n",
    "        E_self.withColumn('dst', F.col('id'))\n",
    "        .withColumnRenamed('id', 'src'))\n",
    "\n",
    "# Create a new graphframe object with labels attached\n",
    "LPAbgf = GraphFrame(v, E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell above, what we did is just assign new label to every user and create a new graphframe object with each node has its label attached. Next, we will define the message that each node send to its neighbors and how the messages are aggregated at each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Number of iteration\n",
    "numIter = 5\n",
    "for iter_ in range(numIter):\n",
    "    for nodeType in [1, 2]:\n",
    "        # For user and repo nodes, send their labels to\n",
    "        # their destination nodes in alternating order\n",
    "        msgForDst = F.when(AM.src['nodeType'] == nodeType,\n",
    "                           AM.src['label'])\n",
    "        # If it's repo's turn to send label to their destinations,\n",
    "        # also send repo's label's to its contributors\n",
    "        if nodeType == 2:\n",
    "            msgForSrc = F.when(AM.src['nodeType'] == 1, AM.dst['label'])\n",
    "        else:\n",
    "            msgForSrc = None\n",
    "\n",
    "        # Aggregate messages received from each node\n",
    "        aggregates = LPAbgf.aggregateMessages(\n",
    "                aggCol = maxLabel_udf(AM.msg).alias(\"aggMess\"),\n",
    "                sendToDst = msgForDst,\n",
    "                sendToSrc = msgForSrc)\n",
    "        v = LPAbgf.vertices\n",
    "\n",
    "        # Update Labels for each node; If there is message for\n",
    "        # the node, update the node's Label\n",
    "        newLabelCol = F.when(aggregates[\"aggMess\"].isNotNull(),\n",
    "                             aggregates[\"aggMess\"]\n",
    "                            ).otherwise(v['label'])\n",
    "        # Outer join aggregates and vertices\n",
    "        vNew = (v\n",
    "            .join(aggregates, on=(v['id'] == aggregates['id']),\n",
    "                how='left_outer').drop(aggregates['id'])\n",
    "            # Compute new column\n",
    "            .withColumn('newLabel', newLabelCol)\n",
    "            # Drop messages\n",
    "            .drop('aggMess')\n",
    "            # Drop old labels\n",
    "            .drop('label')\n",
    "            .withColumnRenamed('newLabel', 'label')\n",
    "        )\n",
    "        # Get the cached data frame of new vertices\n",
    "        # to avoid memory over flow issue\n",
    "        cachedvNew = AM.getCachedDataFrame(vNew)\n",
    "        LPAbgf = GraphFrame(cachedvNew, E)\n",
    "\n",
    "\n",
    "# get rid of the self oriented edges in the data frame\n",
    "LPAbgf = GraphFrame(LPAbgf.vertices,\n",
    "                    graphframe_demo.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a long cell of code... But bear with me. Let's read this code line by line..\n",
    "From the first two line, we can see that for each iteration, we alternatively set user and repo to send their labels to their destination nodes. We use `msgForDst` to define the content of the message that the destination nodes receive. We also specify that when it's repos' turn to send label, also send their label to its contributors. Here, `AM.src` and `AM.dst` are references for columns of source and destination. We can think of them as edge-triplets: source node and destination node are tables that contains information about the nodes in the `vertices_df` table, while the edges in `edges_df` table builds the tunnel for them to exchange information. \n",
    "\n",
    "After we define the content of message, we will define how to aggregate the message at each node. The instance method `aggregateMessages` provided in graphframe gives us an entry point. In the function, we set the argument `aggCol` equal to an sparl sql column expression or a pyspark function/udf that takes in `AM.msg`, which is a column that specifies the content of the messages received by each node. Here, we use the pandas udf that we write in previous section to aggregate the labels received by choosing the most common one. `aggregateMessages` will then return a data frame `aggregates` composed of columns `id` and `aggMess`. \n",
    "\n",
    "Next, we will do left outer join the vertices table `v` with our aggregated message dataframe `aggregates` and update the labels. Finally, using the new vertices table `vnew` and edges `E`, we will create a new graphframe object to either use in the next iteration or to return the final labelling result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime efficiency\n",
    "Speaking of making the code run faster. The above code takes about 22 minutes to run one iteration for Spark running in local mode (16gbs of RAM, 8 logical CPU) for a network with about 6 million nodes and 14 million edges. The Label propagation algorithm provided by graphframe (which is translated GraphX's implementation using RDD and Pregel) use around the same time for one iteration of label updating."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
